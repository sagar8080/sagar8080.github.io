<!DOCTYPE HTML>
<html>
	<head>
		<title>Sagar Das</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<!-- <div class="logo">
							<span class="icon fa-gem"></span>
						</div> -->
						<div class="content">
							<div class="inner">
								<img 
									src="images/profile_picture.png" 
									style="height: 200px; width: 200px; border-radius: 50%; object-fit: cover; display: block; margin: 0 auto;"
								>
								<h1>SAGAR DAS</h1>
								<div class="typewriter-container">
									<span class="typewriter" data-text='[" DATA ENGINEER ", "MLOPS ENGINEER", "SOFTWARE ENGINEER"]'></span>
								</div>
							</div>
						</div>
						
						
						<nav>
							<ul>
								<li><a href="#intro">About Me</a></li>
								<li><a href="#education">Education</a></li>
								<li><a href="#work">Experience</a></li>
								<li><a href="#projects">projects</a></li>
								<li><a href="#skills">Skills</a></li>
							</ul>
						</nav>
					</header>

				<!-- Main -->
					<div id="main">

						<article id="intro">
							
							<div class="core-statement">
								<p>
									Hi, I am Sagar!
								</p>
														
								<div class="career-journey">
									<p> A software engineer passionate about distributed systems and data engineering.</p>
									<p>
										I relish the challenge of building systems that turn raw data into reliable, analytics-ready insights.
										From real-time event processing to cloud-native data pipelines, I specialize in designing scalable architectures using tools like Apache Spark, Iceberg, Kafka, Airflow, and modern cloud platforms (AWS & GCP).
									</p>
									<p>
										Whether it's ingesting millions of clickstream records, deploying secure data apps, or enabling time-travel on petabyte-scale data lakes, I bring a systems-thinking mindset to solve data challenges that power decision-making at scale.
									</p>
									<p>
										When Im not coding, I actively play football and have been avidly supporting Liverpool Football Club since 2005. 
										Occasionally, I enjoy landscape photography and write <a href="https://medium.com/@sgx08">Medium Articles</a>. 
									</p>
								</div>

								<p>
									Let's build something impactful â€” feel free to explore my work or reach out to connect!
								</p>
							</div>
						
							<!-- Resume Download Button -->
							<br />
							<div class="availability-badge">
								<div>
									<h3>Open to offers</h3>
									<p>Roles: Software Engineer | Data Engineer | ML Engineer | AI Engineer | Analytics Engineer</p>
									<a href="https://drive.google.com/file/d/1HsXGMeDSXBfjCUQ2zYgnQlWYdGcopHfw/view?usp=sharing" download="Sagar_Das_Resume.pdf" class="download-button">
										<i class="fas fa-download"></i> Download Resume
									</a>
								</div>
							</div>
						</article>
						<!-- Work -->
						<article id="work">
							<h2 class="major">Work Experience</h2>
							<div class="work-experience-card">
								<span><h3 class="role-title">DATA SPECIALIST - GRADUATE ASSISTANT</h3></span>
								<span class="company-link"><a href="https://it.umd.edu/analytics" >DIVISION OF IT - UMD</a> | COLLEGE PARK, MD | SEP 2023 - MAY 2025</span>
								<div class="achievement-category">
									<li>Building an NLP pipeline to transform surveys into analytics-ready datasets, leveraging PyTorch, LangChain, and HuggingFace Transformers, helping 8 research analysts save 20+ hours collectively on manual data querying efforts</li>
									<li>Deployed R Shiny applications on GCP using ShinyProxy, Docker, and Terraform for multi-user collaboration on internal research applications.</li>
									<li>Incorporated security controls (IAM, RBAC) alongside a Flask application gateway with Google OAuth and reverse-proxy SSL to ensure secure access.</li>
									<li>Prepared a POC using GCP Pub/Sub, Apache Beam, and BigQuery to process 20M+ daily clickstream events from Canvas ELMS, enabling instant classroom analytics for the academic leadership team via Apache Superset dashboards</li>
									<li>Optimized AWS ETL workflows by implementing incremental ingestion and advanced SQL techniques (CTEs, partitioning, indexing) slashing processing time from 7 hrs to 4 hrs while ensuring high data accuracy for reporting.</li>
								</div>
							</div>
							
							<div class="work-experience-card">
								<span><h3 class="role-title">Senior Software Engineer</h3></span>
								<span class="company-link"><a href="https://www.tigeranalytics.com/">TIGER ANALYTICS</a> | CHENNAI, INDIA | JUL 2021 - JUL 2023</span>
								<div class="achievement-category">
									<li>Partnered with data architects to prototype MVP's and launch 2 enterprise solutions: <a href="https://aws.amazon.com/marketplace/pp/prodview-mhhbi7gtwptgk">Intelligent Data Express</a> and <a href="https://aws.amazon.com/marketplace/pp/prodview-hxpwkwaynhgcq">Data Observability Framework.</a></li>
									<li>Developed the backend of the MVP's using AWS serverless tools and later scaled it to a microservice architecture using FastAPI and Docker to support 3X user growth.</li>
									<li>Developed a metadata-driven ETL ingestion framework using Airbyte, Apache Airflow, AWS Glue, and Python, enabling rapid ingestion from diverse enterprise data sources (CDC, streaming, batch, on-premise databases), reducing asset onboarding time from weeks to hours</li>
									<li>Engineered an ACID-compliant Lakehouse using Apache Iceberg, AWS S3, Athena, and Redshift. Implemented SCD Type-2 and time travel capabilities to ensure historical data integrity and support analytical workloads</li>
									<li>Built a data quality tool with Great Expectations, Apache Spark, and Airflow to validate 30+ custom checks on data at rest, reducing data anomalies by 60% across downstream BI and analytics pipelines</li>
									<li>Implemented an infrastructure observability pipeline utilizing CloudWatch logs, ELK stack and Grafana, accelerating root cause analysis and decreasing Mean-Time-To-Resolution (MTTR) for production issues by 40%</li>
									<li>Established data governance capabilities by integrating LinkedIn DataHub, providing detailed tracking, auditability, and visualization of data flow, aiding regulatory compliance (GDPR, CCPA) and trust in data assets</li>
									<li>Collaborated with business analysts and sales teams to translate functional requirements into engineering solutions</li>
									<li>Provided MVP demos and simplified complex technical architectures to clients through concise presentations</li>
								</div>
							</div>
							
							<div class="work-break-note">
								<h6>Took a career break to focus on personal goals and well-being</h6>
							</div>
							
							<div class="work-experience-card">
								<span><h3 class="role-title">Intern & Software Engineer</h3></span>
								<span class="company-link"><a href="https://Xenonstack.com/" >XENONSTACK</a> | CHANDIGARH, INDIA | JAN 2019 - NOV 2019</span>
								<div class="achievement-category">
									<li>Modernized a data platform by migrating legacy Hadoop workflows and Pig scripts to Scala/PySpark ETL jobs on Databricks, processing IoT sensor and weather data from 45 geo-locations via Kafka to create a high-availability data lake powering data science workflows</li>
									<li>Collaborated with MLOPS team to build a Python framework using MLFlow to automate model management & scoring, improving feature engineering efficiency by 33% and reducing model discovery time in production</li>
									<li>Scaled TensorFlow model training by implementing a Ray-based distributed pipeline across a 6-node cluster, reducing computation time by ~10%</li>
									<li>Developed a cost-optimization strategy to bid and select EC2 spot instances for AWS EMR jobs during off-peak hours.</li>
								</div>
							</div>
						</article>


						<!-- About -->
						<article id="education">
							<h2 class="major">Education</h2>
							<div class="education-card">
								<h3>University of Maryland - College Park | USA</h3>
								<div class="degree-details">
									<span class="degree">Master's in Information Management</span>
									<span class="degree">2023-2025 | GPA: 4.0/4.0</span>
								</div>
								<div class="key-coursework">
									<li>Relevant Coursework: Big Data Infrastructure, Data Analytics, Data Integration, Advance Data Science, Cloud Computing, Product Management</li>
									<li>Received complete tuition fee waiver for the entire duration of the degree program</li>
								</div>
								
							</div>
						
							<div class="education-card">
								<h3>Panjab University - Chandigarh | India</h3>
								<div class="degree-details">
									<span class="degree">B.E. Information Technology</span>
									<span class="degree">2015-2019 | GPA: 3.74/4.0</span>
								</div>
								<div class="key-coursework">
									<li>Relevant Coursework: Data Structures and Algorithms, Database Systems, Network Security, Operating Systems, Object Oriented Programming</li>
								</div>
							</div>
						</article>

						<article id="skills">
							<div class="technical-prowess">
								<h2>Core Competencies</h2>
								
								<div class="skill-categories">
									<!-- Column 1: Data Engineering -->
									<div class="skill-column">
										<h5>Data Engineering</h5>
										<ul>
											<li>AWS and GCP cloud tools</li>
											<li>Apache Spark | Apache Beam</li>
											<li>Kafka</li>
											<li>BigQuery | Redshift</i>
											<li>Airflow</li>
											<li>Apache Iceberg | Delta Lake</li>
											<li>Hadoop Ecosystem</li>
											<li>DBT Core</li>
											<li>Apache Superset</li>
											<li>Distributed Systems</li>
										</ul>
									</div>
							
									<!-- Column 2: Software Engineering -->
									<div class="skill-column">
										<h5>Software Engineering</h5>
										<ul>
											<li>Data Structures & Algorithms</li>
											<li>Python | Scala | Rust</li>
											<li>Git | GitOps</li>
											<li>CI/CD Pipelines</li>
											<li>Docker</li>
											<li>Kubernetes</li>
											<li>Terraform</li>
											<li>REST APIs</li>
											<li>Backend Development</li>
											<li>Shell Scripting</li>
											<li>Server Side Programming</li>
											<li>HTML | CSS | Javascript</li>
										</ul>
									</div>
							
									<!-- Column 3: Data Science & MLOps -->
									<div class="skill-column">
										<h5>ML Engineering</h5>
										<ul>
											<li>Feature Engineering</li>
											<li>Supervised & Unsupervised learning</li>
											<li>Pytorch</li>
											<li>Advanced SQL</li>
											<li>Weights & Biases</li>
											<li>Scikit-learn</li>
											<li>XGBoost</li>
											<li>MLflow</li>
											<li>Kubeflow</li>
											<li>Metaflow</li>
											<li>SparkML</li>
											<li>Model Serving and Management</li>
										</ul>
									</div>
							
									<!-- Column 4: GenAI -->
									<div class="skill-column">
										<h5>Generative AI</h5>
										<ul>
											<li>LangChain</li>
											<li>Natural Language Processing</li>
											<li>Vector DBs</li>
											<li>RAG Architecture</li>
											<li>LLM and Fine-tuning</li>
											<li>Prompt Engineering</li>
											<li>Hugging Face Transformers</li>
											<li>Agentic AI systems</li>
										</ul>
									</div>
								</div>
							</div>
						</article>
						

						<article id="projects">
							<h2 class="major">Selected Works</h2>

							<div class="project-card">
								<h3>Data Fusion Engineering</h3>
								<div class="tech-stack">
									<span>GCP Serverless</span>
									<span>Apache Spark</span>
									<span>Terraform</span>
									<span>Apache Superset</span>
									<span>BASH</span>
									<span>SQL</span>
								</div>
								<ul>
									<li>Developed a complete analytics solution on GCP to ingest, store, transform, and analyze data</li>
									<li>Automated data ingestion using time-driven cloud function from 6 Open NYC dataset APIs incrementally</li>
									<li>Created automated Dataproc pipelines to process and transform ingested data into a BigQuery</li>
									<li>Prepared auto-updating Apache Superset dashboards to visualize KPIs to identify accident-prone areas</li>
								</ul>
								<a href="https://github.com/sagar8080/data-fusion-engineering/" class="project-link">View on GitHub â†’</a>
							</div>
						
							<div class="project-card">
								<h3>Intelligent Record Management</h3>
								<div class="tech-stack">
									<span>PyTorch</span>
									<span>NLP Tools</span>
									<span>Langchain</span>
									<span>Streamlit</span>
									<span>Elasticsearch</span>
									<span>Gemma2</span>
								</div>
								<ul>
									<li>A document processing and semantic search system for intelligent indexing of congressional archives</li>
									<li>Allows users to input a query via Streamlit UI and retrieve relevant past press releases</li>
									<li>LLM based document summaries infused with NLP: Topic Modeling, NER, and keyword extraction</li>
								</ul>
								<a href="https://github.com/sagar8080/semantic-search-system" class="project-link">View on GitHub â†’</a>
							</div>

							<div class="project-card">
								<h3>Loan Default Prediction System</h3>
								<div class="tech-stack">
									<span>PySpark</span>
									<span>Seaborn</span>
									<span>SciKit Learn</span>
									<span>XGBoost</span>
									<span>Random Forest</span>
									<span>K-Means Clustering</span>
									<span>PCA</span>
								</div>
								<ul>
									<li>End-to-end machine learning pipeline to predict loan defaults using the LendingClub dataset</li>
									<li>Incorporated data preprocessing, feature engineering, & supervised ML modeling</li>
									<li>Prepared a borrower segmentation using K-Means clustering to identify high-risk defaulter profiles</li>
								</ul>
								<a href="https://github.com/sagar8080/loan-default-prediction" class="project-link">View on GitHub â†’</a>
							</div>

							<div class="project-card">
								<h3>Data Preparation for Fintech Analytics</h3>
								<div class="tech-stack">
									<span>AWS</span>
									<span>Python</span>
									<span>Great Expectations</span>
									<span>Postgres</span>
									<span>Tableau</span>
								</div>
								<ul>
									<li>A serverless framework to automate metadata extraction, profiling, validation, and transformation.</li>
									<li>Event-driven workflows on AWS Lambda, Step Functions, and S3 for DQ checks and transformations.</li>
									<li>Stored cleaned datasets into AWS RDS and created Tableau dashboards to visualize KPIs</li>
								</ul>
								<a href="https://github.com/sagar8080/data-prep-for-fintech-loan-analytics" class="project-link">View on GitHub â†’</a>
							</div>

							<div class="project-card">
								<h3>Monitoring EKS Cluster</h3>
								<div class="tech-stack">
									<span>AWS</span>
									<span>Terraform</span>
									<span>HELM</span>
									<span>Prometheus</span>
									<span>Jenkins CI/CD</span>
									<span>Kubernetes</span>
								</div>
								<ul>
									<li>Streamlined the setup for deploying OpenTelemetry Webshop</li>
									<li>Enabled modular deployment by splitting large Kubernetes manifests into component YAML files</li>
									<li>Provisioned EKS, Grafana, and Prometheus with Terraform and Helm</li>
									<li>Developed a system to collect logs from the kube-system and track unhealthy pods</li>
									<li>Built pipelines to test and build Docker images via a remote Jenkins server</li>
								</ul>
								<a href="https://github.com/sagar8080/eks-and-monitoring" class="project-link">View on GitHub â†’</a>
							</div>
						
							<div class="project-card">
								<h3>Sports Analytics System</h3>
								<div class="tech-stack">
									<span>Python</span>
									<span>Plotly</span>
									<span>Pandas</span>
									<span>Tableau</span>
								</div>
								<ul>
									<li>Developed EDA framework analyzing 10+ performance KPIs</li>
									<li>Created interactive dashboards for tactical analysis</li>
									<li>Identified 3 key success factors through Bayesian analysis</li>
								</ul>
								<a href="https://www.kaggle.com/code/sagardas96/das-inst627-fall-2023" class="project-link">View on Kaggle â†’</a>
							</div>
						</div>
						</article>
						
						<br>
					<div>
						<div>
							<div>
								<ul class="icons" style="text-align: center;">
									<li><a href="https://www.linkedin.com/in/sagardas08/" class="icon brands fa-linkedin-in"><span class="label"></span></a></li>
									<li><a href="https://github.com/sagar8080" class="icon brands fa-github"><span class="label"></span></a></li>
									<li><a href="mailto:sagardas.work@gmail.com?subject=Hey Sagar, I am reaching out to you regarding: & body=Message" class="fa fa-envelope"><span class="label"></span></a></li>
									<li><a href="tel:+1-2404959874" class="fa fa-mobile-alt"><span class="label"></span></a></li>
									<li><a href="https://leetcode.com/sagar8080/" class="fa fa-code"><span class="label"></span></a></li>										  
								</ul>
							</div>
						</div>
						<div>
							<footer id="footer">
								<p class="copyright">&copy; Sagar Das 2025</p>
							</footer>
						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>